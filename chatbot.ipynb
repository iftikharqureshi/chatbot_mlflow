{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dafc2a",
   "metadata": {},
   "source": [
    "# Chatbot with MLflow Tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face6d5c",
   "metadata": {},
   "source": [
    "## Setting Up the Core Libraries for the Chatbot and Tracking System\n",
    "\n",
    "In this section, I start by setting up the essential tools that make the rest of the notebook come alive. The idea is to build a small interactive chatbot that talks through an OpenAI model like GPT-3.5-Turbo while quietly keeping track of everything happening in the background using MLflow. To make that possible, I import a few important libraries. The `os` and `time` modules help manage system operations and measure timing details. `mlflow` is what I use to log and track all the key metrics from each chat session, such as response time, tokens used, and model parameters. The `openai` package provides direct access to the language model itself, while `tiktoken` helps count tokens so I can understand how much text is being processed. Finally, I bring in `colorama` to add a touch of color to the console output, making the chatbot responses more readable and visually engaging. Together, these imports form the foundation for an AI-powered chat experience that‚Äôs both interactive and measurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a622ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from openai import OpenAI\n",
    "import tiktoken as tk\n",
    "from colorama import Fore, Style, init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa63cab",
   "metadata": {},
   "source": [
    "## Defining Core Configuration and Model Parameters\n",
    "\n",
    "Here, I define all the key settings that control how the chatbot behaves and how MLflow connects to the tracking server. The first line pulls my OpenAI API key from an environment variable, which keeps it secure instead of hardcoding it directly in the notebook. Next, I set the model name to `gpt-3.5-turbo`, which will handle all the chat responses. The `MLFLOW_URI` points to my local MLflow server where the tracking data will be stored and viewed later. Then I configure several tuning parameters such as `TEMPERATURE`, `TOP_P`, `FREQUENCY_PENALTY`, and `PRESENCE_PENALTY`, which influence how creative or consistent the model‚Äôs responses are. The `MAX_TOKENS` value limits how long the responses can be, while `DEBUG` lets me toggle extra logging for troubleshooting if needed. Together, these constants define the chatbot‚Äôs behavior and ensure the system logs each conversation under the right experiment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446653a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and configuration parameters\n",
    "API_KEY = os.getenv(\"OPENAI_API_BOOK_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "MLFLOW_URI = \"http://localhost:5000\"\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 1\n",
    "FREQUENCY_PENALTY = 0\n",
    "PRESENCE_PENALTY = 0\n",
    "MAX_TOKENS = 800\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189f8c9",
   "metadata": {},
   "source": [
    "## Initializing Colorama for Colored Console Output\n",
    "\n",
    "In this step, I initialize `colorama`, a lightweight Python library that adds color and styling to text displayed in the terminal. By calling `init()`, I make sure the color codes work properly across different operating systems, especially on Windows where console color support can vary. This small setup step helps make the chatbot‚Äôs messages stand out visually, allowing prompts, responses, and status updates to appear in different colors for better readability during live interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d510f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize colorama for colored console output\n",
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf085580",
   "metadata": {},
   "source": [
    "## Connecting to the OpenAI API\n",
    "\n",
    "At this point, I create an instance of the OpenAI client using my stored API key. This step is what actually connects the notebook to OpenAI‚Äôs servers and allows me to send messages to the model and receive responses in return. By initializing the client here, I make sure that every later request‚Äîwhether it‚Äôs a simple chat prompt or a logged conversation‚Äîcan securely communicate with the model I configured earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b1baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c688ee0",
   "metadata": {},
   "source": [
    "## Configuring MLflow Tracking and Experiment Setup\n",
    "\n",
    "Here, I connect MLflow to the tracking server and define the experiment where all my chatbot sessions will be logged. The first line sets the tracking URI, which tells MLflow where to send and store the recorded data‚Äîsuch as metrics, parameters, and artifacts. The next line creates or switches to an experiment named `\"GenAI_Week9\"`, grouping all the related runs under a single project space. This setup ensures that each chat interaction is properly organized, making it easy for me to compare performance, tune parameters, and review results later through the MLflow dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26233d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/08 14:00:01 INFO mlflow.tracking.fluent: Experiment with name 'GenAI_Week9' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/2', creation_time=1762632001448, experiment_id='2', last_update_time=1762632001448, lifecycle_stage='active', name='GenAI_Week9', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(\"GenAI_Week9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dfb72",
   "metadata": {},
   "source": [
    "## Creating Helper Functions for Colorful Chat Display\n",
    "\n",
    "In this part, I define two simple helper functions that make the chat interface more readable by adding color to the console text. The first function, `print_user_input()`, displays anything I type in green, clearly marking it as the user‚Äôs message. The second one, `print_ai_output()`, prints the AI‚Äôs responses in blue, so it‚Äôs easy to distinguish between who said what during the conversation. Using these color cues not only makes the interaction feel more natural but also helps when reviewing logs or running multiple chat sessions in the same terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b46161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for colored text display\n",
    "def print_user_input(text):\n",
    "    print(f\"{Fore.GREEN}You: {Style.RESET_ALL}\", text)\n",
    "\n",
    "def print_ai_output(text):\n",
    "    print(f\"{Fore.BLUE}AI Assistant:{Style.RESET_ALL}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa431c",
   "metadata": {},
   "source": [
    "## Adding a Token Counter for Measuring Text Length\n",
    "\n",
    "In this section, I define a function that counts how many tokens a given piece of text contains. Tokens are the small text chunks that language models process, and knowing how many are used helps track cost, performance, and response size. The function first loads a specific tokenizer using `tiktoken`, which is designed to match the encoding used by OpenAI models. It then converts the input string into its tokenized form and simply returns the number of tokens found. This small utility will later help me log token usage for each chat message and analyze how efficiently the model is generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce01c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to count tokens using tiktoken\n",
    "def count_tokens(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    # Get encoding for the provided name\n",
    "    encoding = tk.get_encoding(encoding_name)\n",
    "    \n",
    "    # Encode the string to tokens\n",
    "    encoded_string = encoding.encode(string, disallowed_special=())\n",
    "    \n",
    "    # Count number of tokens\n",
    "    num_tokens = len(encoded_string)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63802532",
   "metadata": {},
   "source": [
    "## Building the Core Function to Generate and Log AI Responses\n",
    "\n",
    "This function is the heart of the chatbot‚Äîit handles sending messages to the OpenAI model, receiving responses, measuring performance, and logging everything through MLflow. When I call `generate_text()`, it starts by recording the current time so I can later calculate how long the model took to reply. The function then sends the full conversation history to the model using the `ChatCompletion` API, along with the configured parameters that shape the tone and creativity of the response. Once the model replies, I measure the total latency and extract the AI‚Äôs message. To keep track of efficiency, I count tokens for the latest user input, the entire conversation, and the model‚Äôs response. These metrics, along with the model configuration details, are logged to MLflow so I can analyze patterns like response time or token usage over different runs. If debugging is enabled, I can even pause to inspect the active run ID before continuing. In the end, the function returns the AI-generated text, completing one full cycle of interaction, measurement, and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1cfac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate text using OpenAI API\n",
    "def generate_text(conversation, max_tokens=100) -> str:\n",
    "    # Record start time for latency calculation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call the OpenAI ChatCompletion API\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=conversation,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=TOP_P,\n",
    "        frequency_penalty=FREQUENCY_PENALTY,\n",
    "        presence_penalty=PRESENCE_PENALTY\n",
    "    )\n",
    "    \n",
    "    # Measure total latency\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # Extract message content\n",
    "    message_response = response.choices[0].message.content\n",
    "    \n",
    "    # Count token usage\n",
    "    prompt_tokens = count_tokens(conversation[-1]['content'])\n",
    "    conversation_tokens = count_tokens(str(conversation))\n",
    "    completion_tokens = count_tokens(message_response)\n",
    "    \n",
    "    # Retrieve the active MLflow run\n",
    "    run = mlflow.active_run()\n",
    "    \n",
    "    # Display debug information if enabled\n",
    "    if DEBUG:\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        input(\"Press Enter to continue...\")\n",
    "    \n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metrics({\n",
    "        \"request_count\": 1,\n",
    "        \"request_latency\": latency,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"conversation_tokens\": conversation_tokens\n",
    "    })\n",
    "    \n",
    "    # Log model parameters to MLflow\n",
    "    mlflow.log_params({\n",
    "        \"model\": MODEL,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"frequency_penalty\": FREQUENCY_PENALTY,\n",
    "        \"presence_penalty\": PRESENCE_PENALTY\n",
    "    })\n",
    "    \n",
    "    # Return AI-generated message\n",
    "    return message_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac0c69",
   "metadata": {},
   "source": [
    "## Enabling MLflow Autologging for Automatic Tracking\n",
    "\n",
    "Here, I turn on MLflow‚Äôs autologging feature, which automatically captures important information during each run without requiring extra manual code. With this enabled, MLflow keeps track of parameters, metrics, and artifacts generated by the model or supporting libraries. It acts as a safety net that ensures no key data is missed, even if I forget to log something explicitly. This makes the experiment tracking smoother and provides a more complete record of each chatbot interaction for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae122de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/08 14:00:01 INFO mlflow.tracking.fluent: Autologging successfully enabled for openai.\n"
     ]
    }
   ],
   "source": [
    "# Enable MLflow autologging\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd598089",
   "metadata": {},
   "source": [
    "## Enabling MLflow Autologging for Automatic Tracking\n",
    "\n",
    "Here, I turn on MLflow‚Äôs autologging feature, which automatically captures important information during each run without requiring extra manual code. With this enabled, MLflow keeps track of parameters, metrics, and artifacts generated by the model or supporting libraries. It acts as a safety net that ensures no key data is missed, even if I forget to log something explicitly. This makes the experiment tracking smoother and provides a more complete record of each chatbot interaction for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54448bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Assistant: In traditional programming, a developer writes explicit instructions that tell a computer how to perform a specific task. The developer needs to anticipate all possible scenarios and write code to handle them accordingly. The program follows these predefined rules to produce the desired output.\n",
      "\n",
      "On the other hand, in machine learning, a computer is trained to learn from data without being explicitly programmed for specific tasks. Instead of writing explicit instructions, a machine learning algorithm learns patterns and relationships from the data provided to it. The model created during the training phase can then make predictions or decisions based on new, unseen data.\n",
      "\n",
      "In summary, the main difference is that traditional programming relies on explicit instructions provided by the developer, while machine learning leverages data to learn and make decisions. Machine learning is particularly useful for tasks where it is difficult to explicitly define rules or where the data is too complex for traditional programming approaches.\n",
      "üèÉ View run sedate-smelt-545 at: http://localhost:5000/#/experiments/2/runs/b403bf05a7f64134b56fc8b5ead2e58b\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-da7206ca5a4ed3ed85a441a384d57e5f&amp;experiment_id=2&amp;version=3.5.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-da7206ca5a4ed3ed85a441a384d57e5f)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start an MLflow run and interact with the AI assistant\n",
    "with mlflow.start_run() as run:\n",
    "    # Initialize the system message for conversation context\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    ]\n",
    "    \n",
    "    # Enter interactive conversation loop\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        \n",
    "        # Exit loop on specific user commands\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"q\", \"e\"]:\n",
    "            break\n",
    "        \n",
    "        # Append user message to conversation\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Generate AI response\n",
    "        ai_output = generate_text(conversation, MAX_TOKENS)\n",
    "        \n",
    "        # Display AI response with color\n",
    "        print_ai_output(ai_output)\n",
    "        \n",
    "        # Add AI response to conversation history\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": ai_output})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f3c62",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "When I asked the question, \"Explain how machine learning differs from traditional programming,\" the model delivered a clear and accurate explanation that effectively contrasted rule-based programming with data-driven learning. The model‚Äôs response clearly demonstrates that both the chatbot and MLflow tracking are functioning as intended. It provides an accurate and well-organized explanation that distinguishes traditional programming from machine learning in a simple, logical way. The tone is clear and instructive, showing that the model can communicate technical ideas effectively. The successful logging of metrics, visible run details, and smooth display formatting confirm that the full interaction pipeline‚Äîfrom user input to AI response and experiment tracking‚Äîworks seamlessly, making this an effective first validation of the system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
